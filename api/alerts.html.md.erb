---
title: Configuring PCF Healthwatch Alerts
owner: PCF Healthwatch
---

This topic describes how to use the Pivotal Cloud Foundry (PCF) Healthwatch API to retrieve and configure alert configurations. It also provides information about configuring PCF Event Alerts to receive push notifications when a PCF Healthwatch alert occurs. 

Currently, two main types of alert configurations are supported: Out-of-the-Box and Deployment-specific. Through this API, consumers can do the following:

- View current alert configurations
- Update Out-of-the-Box threshold values
- Create or delete deployment-specific configurations based on existing Out-of-the-Box configurations

##<a id='prerequisites'></a>Prerequisites/Assumptions

The steps in this document assume that you can [generate bearer tokens for a UAA client](https://docs.pivotal.io/pivotalcf/uaa/uaa-user-management.html) with the `healthwatch.read` (`GET` only) and `healthwatch.admin` (both `GET` and `POST`) scopes.

After creating a user that has `healthwatch.read` or `healthwatch.admin` scopes, follow these steps to authenticate against UAA:

```
uaac token client get <my_healthwatch_admin_client> -s <my_healthwatch_admin_secret>
```

At this point you are properly authenticated and ready to start using the Healthwatch Alerts API.

##<a id='info'></a>Healthwatch API Status

Test the availability of the Healthwatch API by hitting the `/info` endpoint with a `GET` request:

```
curl https://healthwatch-api.SYSTEM-DOMAIN/info
```

The expected response is a `200`/`OK` with the message `"HAPI is happy"`.

##<a id='get'></a>View All Alert Configurations
### `GET /v1/alert-configurations`

To view a list of alert configurations, send a `GET` request to the `/alert-configurations` endpoint:

```
uaac curl https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations
```

This returns a JSON array of alert configurations:

```
[
    {
        "query": "origin == 'some_origin' and name == 'Some.Metric.Name'",
        "threshold": {
            "critical": 95,
            "warning": 85,
            "type": "UPPER"
        }
    },
    {
        "query": "origin == 'some_origin' and name == 'Another.Metric.Name'",
        "threshold": {
            "critical": 9,
            "warning": 28,
            "type": "LOWER"
        }
    },
    {
        "query": "origin == 'another_origin' and name == 'Some.Metric.Name'",
        "threshold": {
            "critical": 1,
            "type": "EQUALITY"
        }
    }
]
```

The `query` and `threshold` properties are covered in detail below.

##<a id='get_with_query'></a>View Specific Alert Configurations
### `GET /v1/alert-configurations?q=...`

To narrow the results of a `GET` request, add a `query` to the URL in a parameter named `q`:

```
uaac curl "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations?q=origin == 'some_origin' and name == 'Some.Metric.Name'" 
```

This returns a JSON array of alert configurations, filtered against the provideded `query`:

```
[
    {
        "query": "origin == 'some_origin' and name == 'Some.Metric.Name'",
        "threshold": {
            "critical": 95,
            "warning": 85,
            "type": "UPPER"
        }
    }
]
```

##<a id='post'></a>Update Alert Configurations
### `POST /v1/alert-configurations`

To update an existing alert configuration, make a `POST` request to the `alert-configurations` endpoint with the updated data:

```
uaac curl -X POST "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations"  \
      -H "Content-Type: application/json" \
      --data "{\"query\":\"origin == 'some_origin' and name == 'Some.Metric.Name'\",\"threshold\":{\"critical\":90,\"warning\":80,\"type\":\"UPPER\"}}"

```

See the following example output:

```
{
    "query": "origin == 'some_origin' and name == 'Some.Metric.Name'",
    "threshold": {
        "critical": 95,
        "warning": 85,
        "type": "UPPER"
    }
}
```

<p class="note warning"><strong>Warning</strong>: These alert configurations cannot be deleted. In order to revert your changes, update the alert back to its <a href="#defaults">default values</a>.</p>  

##<a id='post2'></a>Create Alert Configurations for Isolation Segments
### `POST /v1/alert-configurations`

Specific thresholds can be set for an Isolation Segments by extending existing alert configurations with a deployment specifier. For example, to create an isolation segment alert configuration for the above alert, run the following:

```
uaac curl -X POST "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations"  \
      -H "Content-Type: application/json" \
      --data "{\"query\":\"origin == 'some_origin' and name == 'Some.Metric.Name' and deployment == 'Some-Isolated-Deployment'\",\"threshold\":{\"critical\":55,\"warning\":45,\"type\":\"UPPER\"}}"
```

The created alert configuration is echoed back in the following response:

```
{
    "query": "origin == 'some_origin' and name == 'Some.Metric.Name' and deployment == 'Some-Isolated-Deployment'",
    "threshold": {
        "critical": 55,
        "warning": 45,
        "type": "UPPER"
    }
}
```
<p class="note"><strong>Note</strong>: You can delete Isolation Segment alert configurations only if you created them with the above method.</p>

##<a id='delete'></a>Delete Isolation Segment Alert Configurations

To delete a user-created alert configuration for an isolation segment, add a `query` to the URL in a parameter named `q`. 

`DELETE /v1/alert-configurations?q=...`

See the following example:

```
uaac curl -X DELETE "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations?q=origin == 'some_origin' and name == 'Some.Metric.Name' and deployment == 'Some-Isolated-Deployment'"
```

This returns the number of deleted alert configurations. See the following example output.

```
1
```

<p class="note"><strong>Note</strong>: You can delete Isolation Segment alert configurations only if you created them through the Healthwatch API.</p>

## <a id='disable'></a> Disable Alerts on a Metric

PCF Healthwatch does not support disabling alerts on a specific metric. To ensure that PCF Healthwatch does not alert on a metric, update the threshold of the alert to a value that will never trigger an alert.

For more information about thresholds, see [Thresholds](#thresholds).

For more information about updating alert configurations, see [Update Alert Configurations](#post).

##<a id='queries'></a>Queries

The `query` field is used in two ways:

* `GET` requests: The `query` filters the alert configurations being queried.
* `POST` requests: The `query` specifies the alert configuration being updated.
* `DELETE` requests: The `query` filters the alert configurations being deleted.

The `query` associated with an alert configuration denotes the trigger conditions for the alert.

A well-formed `query` is a valid [Spring Expression Language (SpEL)](https://docs.spring.io/spring/docs/4.3.14.RELEASE/spring-framework-reference/html/expressions.html) expression that uses only the equality, `"=="`, and conjunction, `"and"`, operators. For example:

```
"origin == 'some_origin' and name == 'Some.Metric.Name'"  # valid
"origin  > 'some_origin' and name == 'Some.Metric.Name'"  # invalid (uses '>')
"origin == 'some_origin'  or name == 'Some.Metric.Name'"  # invalid (uses 'or')
```

The following fields can be queried:

- `origin`&nbsp;&nbsp;&nbsp;*(required)*
- `name`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*(required)*
- `job`
- `deployment`

##<a id='thresholds'></a>Thresholds

The `threshold` field contains a threshold `type`, as well as `critical` and `warning` threshold values. The `type` can be `"UPPER"`, `"LOWER"`, `"EQUALITY"`, or `"INEQUALITY"`.

Alert configurations whose thresholds are of the `UPPER` type trigger their alerts when the actual metric value is *above* the `warning` value, and again when above the `critical` value.

Thresholds with the `LOWER` type work the same way, except the alerts trigger when the metric falls *below* the thresholds.

The `EQUALITY` alerts trigger when the metric value is **not** exactly equal to the `critical` threshold. These alerts do not have `warning` thresholds.

The `INEQUALITY` alerts trigger when the metric value **is** exactly equal to the `critical` threshold. These alerts do not have `warning` thresholds.

##<a id='defaults'></a>Supported Alerts

This section describes the default Warning and Critical alerts for PCF Healthwatch. Each alert includes recommended thresholds for metrics monitored by Healthwatch. 

Pivotal recommends customizing the default thresholds for alerts indicated with a <sup>1</sup> in the following table based on your environment. You can determine the best threshold for your environment by monitoring the metrics over time and noting the metric values that indicate acceptable and unacceptable system performance and health. For more information about updating alert configurations in PCF Healthwatch, see [Update Alert Configurations](#post).

By default, PCF Healthwatch includes the following configurable alerts:

| Description                                                                | Metric Name                                        | Metric Origin                 | Job                     | Category            | Threshold Type | Critical     | Warning    | Unit    | Assessment Window (minutes) |
|----------------------------------------------------------------------------|----------------------------------------------------|-------------------------------|-------------------------|---------------------|----------------|--------------|------------|---------|-----------------------------|
| Performance Alert: Active Locks Held                                       | ActiveLocks	                                      | locket	                      |                         | Compute Performance | EQUALITY       | 4            |            | Number  | 5                           |
| Performance Alert: Active Presences Held<sup>1</sup>                       | ActivePresences	                                  | locket	                      |                         | Compute Performance | UPPER          | 200          | 150        | Number  | 15                          |
| Performance Alert: Auctioneer Time to Fetch Cell State                     | AuctioneerFetchStatesDuration                      | auctioneer                    |                         | Compute Performance | UPPER          | 5000000000   | 2000000000 | ns      | 5                           |
| Performance Alert: App Instances Placement Failures Rate                   | AuctioneerLRPAuctionsFailed                        | auctioneer                    |                         | App Instances       | UPPER          | 1            | .5         | Number  | 5                           |
| Performance Alert: App Instance Starts Rate<sup>1</sup>                    | AuctioneerLRPAuctionsStarted                       | auctioneer                    |                         | App Instances       | UPPER          | 100          | 50         | Number  | 5                           |
| Performance Alert: Router Exhausted Connections<sup>1</sup>                | backend\_exhausted\_conns                          | gorouter                      |                         | Routing             | UPPER          | 10           | 5          | Number  | 5                           |
| Performance Alert: Number of Router 502 Bad Gateways<sup>1</sup>           | bad_gateways                                       | gorouter                      |                         | Routing             | UPPER          | 40           | 30         | Number  | 5                           |
| Performance Alert: Task Placement Failures Rate                            | AuctioneerTaskAuctionsFailed                       | auctioneer                    |                         | App Instances       | UPPER          | 1            | .5         | Number  | 5                           |
| Performance Alert: BBS Time to Run LRP Convergence                         | ConvergenceLRPDuration                             | bbs                           |                         | Compute Performance | UPPER          | 20000000000  | 10000000000| ns      | 15                          |
| Performance Alert: Number of Crashed App Instances<sup>1</sup>             | CrashedActualLRPs                                  | bbs                           |                         | App Instances       | UPPER          | 20           | 10         | Number  | 5                           |
| Performance Alert: Cloud Controller and Diego in Sync                      | Diego.AppsDomainSynced                             | bbs                           |                         | Compute Performance | EQUALITY       | 1            |            | Number  | 5                           |
| Scaling Alert: Number of Available Free Chunks of Cell Memory              | Diego.AvailableFreeChunks                          | healthwatch                   |                         | Capacity            | LOWER          | 1            | 2          | Number  | 5                           |
| Scaling Alert: Number of Available Free Chunks of Cell Disk                | Diego.AvailableFreeChunksDisk                      | healthwatch                   |                         | Capacity            | LOWER          | 50           | 100        | Number  | 5                           |
| Performance Alert: Rate of Change in Running App Instances<sup>1</sup>     | Diego.LRPsAdded.1H                                 | healthwatch                   |                         | App Instances       | UPPER          | 100          | 50         | Number  | 5                           |
| Scaling Alert: Remaining Cell Disk Available                               | Diego.TotalAvailableDiskCapacity.5M                | healthwatch                   |                         | Capacity            | LOWER          | 6144         | 12288      | MBs     | 5                           |
| Scaling Diego: Remaining Cell Memory Available<sup>1</sup>                 | Diego.TotalAvailableMemoryCapacity.5M              | healthwatch                   |                         | Capacity            | LOWER          | 32768        | 65536      | MBs     | 5                           |
| Scaling Alert: Cell Container Capacity Available                           | Diego.TotalPercentageAvailableContainerCapacity.5M | healthwatch                   |                         | Capacity            | LOWER          | 0.35         | 0.4        | Percent | 30                          |
| Scaling Alert: Cell Disk Available                                         | Diego.TotalPercentageAvailableDiskCapacity.5M      | healthwatch                   |                         | Capacity            | LOWER          | 0.35         | 0.4        | Percent | 30                          |
| Scaling Alert: Cell Memory Available<sup>1</sup>                           | Diego.TotalPercentageAvailableMemoryCapacity.5M    | healthwatch                   |                         | Capacity            | LOWER          | 0.35         | 0.4        | Percent | 30                          |
| Scaling Alert: Doppler Message Rate Capacity                               | Doppler.MessagesAverage.1M                         | healthwatch                   |                         | Logging             | UPPER          | 1000000      | 800000     | Number  | 60                          |
| Performance Alert: Router File Descriptors                                 | file_descriptors                                   | gorouter                      |                         | Routing             | UPPER          | 60000        | 50000      | Number  | 5                           |
| Scaling Alert: Log Transport Loss Rate                                     | Firehose.LossRate.1M                               | healthwatch                   |                         | Logging             | UPPER          | 0.01         | 0.005      | Percent | 5                           |
| Performance Alert: PAS MySQL Galera Cluster Status                         | Galera.ClusterStatusSum                            | healthwatch                   |                         | MySQL               | LOWER          | 0.9999       | 2.9999     | Number  | 5                           |
| Performance Alert: PAS MySQL Galera Cluster Size                           | Galera.TotalPercentageHealthyNodes                 | healthwatch                   |                         | MySQL               | LOWER          | 0.3332       | 0.9999     | Percent | 5                           |
| Performance Alert: Healthwatch BOSH Director Test Availability             | health.check.bosh.director.probe.available         | healthwatch                   |                         | BOSH Director       | LOWER          | 0.4          | 0.6        | Number  | 10                          |
| Service Level Indicator Alert: BOSH Director Health                        | health.check.bosh.director.success                 | healthwatch                   |                         | BOSH Director       | EQUALITY       | 1            |            | Number  | 10                          |
| Service Level Indicator Alert: Ops Manager Availability                    | health.check.OpsMan.available                      | healthwatch                   |                         | Ops Manager         | LOWER          | 0.5          |            | Number  | 5                           |
| Performance Alert: Ops Manager Test Availability                           | health.check.OpsMan.probe.available                | healthwatch                   |                         | Ops Manager         | LOWER          | 0.4          | 0.6        | Number  | 5                           |
| Service Level Indicator Alert: Canary App Availability                     | health.check.CanaryApp.available                   | healthwatch                   |                         | Canary App          | LOWER          | 0.5          |            | Number  | 5                           |
| Performance Alert: Canary App Health Test Availability                     | health.check.CanaryApp.probe.available             | healthwatch                   |                         | Canary App          | LOWER          | 0.4          | 0.6        | Number  | 5                           |
| Service Level Indicator Alert: Canary App Response Time                    | health.check.CanaryApp.responseTime                | healthwatch                   |                         | Canary App          | UPPER          | 30000        | 15000      | ms      | 5                           |
| Service Level Indicator Alert: CF Push Time                                | health.check.cliCommand.pushTime                   | healthwatch                   |                         | CLI                 | UPPER          | 120000       | 60000      | ms      | 10                          |
| Service Level Indicator Alert: Can CF Login                                | health.check.cliCommand.login                      | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Service Level Indicator Alert: Can CF Push                                 | health.check.cliCommand.push                       | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Service Level Indicator Alert: Can CF Start                                | health.check.cliCommand.start                      | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Service Level Indicator Alert: Can CF Stop                                 | health.check.cliCommand.stop                       | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Service Level Indicator Alert: Can CF Delete                               | health.check.cliCommand.delete                     | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Service Level Indicator Alert: Can Receive Logs                            | health.check.cliCommand.logs                       | healthwatch                   |                         | CLI                 | INEQUALITY     | 0            |            | Number  | 10                          |
| Performance Alert: CLI Health Test Availability                            | health.check.cliCommand.probe.available            | healthwatch                   |                         | CLI                 | LOWER          | 0.4          | 0.6        | Number  | 5                           |
| Performance Alert: Healthwatch UI Availability                             | health.check.ui.available                          | healthwatch                   |                         | Healthwatch         | LOWER          | 0.4          | 0.6        | Number  | 5                           |
| Performance Alert: Healthwatch Nozzle Disconnects                          | ingestor.disconnects                               | healthwatch                   |                         | Healthwatch         | UPPER          | 10           | 5          | Number  | 5                           |
| Performance Alert: Healthwatch Ingestor Data Drops                         | ingestor.dropped                                   | healthwatch                   |                         | Healthwatch         | UPPER          | 20           | 10         | Number  | 5                           |
| Performance Alert: Healthwatch Ingestor Metrics Ingested                   | ingestor.ingested                                  | healthwatch                   |                         | Healthwatch         | LOWER          | 0.01         | 0.01       | Number  | 30                          |
| Performance Alert: Healthwatch Ingestor BOSH System Metrics Ingested       | ingestor.ingested.boshSystemMetrics                | healthwatch                   |                         | Healthwatch         | LOWER          | 0.01         | 0.01       | Number  | 30                          |
| Performance Alert: Router Handling Latency<sup>1</sup>                     | latency                                            | gorouter                      |                         | Routing             | UPPER          | 150          | 100        | ms      | 30                          |
| Performance Alert: UAA Request Latency                                     | latency.uaa                                        | gorouter                      |                         | Routing             | UPPER          | 150          | 100        | ms      | 5                           |
| Performance Alert: Locks Held by BBS                                       | LockHeld                                           | bbs                           |                         | Healthwatch         | EQUALITY       | 1            |            | Number  | 5                           |
| Performance Alert: Locks Held by Auctioneer                                | LockHeld                                           | auctioneer                    |                         | Compute Performance | EQUALITY       | 1            |            | Number  | 5                           |
| Performance Alert: More App Instances Than Expected                        | LRPsExtra                                          | bbs                           |                         | App Instances       | UPPER          | 10           | 5          | Number  | 5                           |
| Performance Alert: Fewer App Instances Than Expected                       | LRPsMissing                                        | bbs                           |                         | App Instances       | UPPER          | 10           | 5          | Number  | 5                           |
| Performance Alert: Healthwatch Super Metrics Published                     | metrics.published                                  | healthwatch                   |                         | Healthwatch         | LOWER          | 0            | 20         | Number  | 5                           |
| Performance Alert: Time Since Last Route Register Received                 | ms\_since\_last\_registry\_update                  | gorouter                      |                         | Routing             | UPPER          | 30000        | 30000      | ms      | 5                           |
| Performance Alert: PAS MySQL Server Availability                           | /mysql/available                                   | mysql                         | mysql,database          | MySQL               | EQUALITY       | 1            |            | Number  | 5                           |
| Performance Alert: PAS MySQL Galera Cluster Node Readiness                 | /mysql/galera/wsrep_ready	                      | mysql	                      | mysql,database          | MySQL               | LOWER          | 1            | 0.9999     | Number  | 5                           |
| Scaling Alert: Redis Counter Event Queue Size                              | redis.counterEventQueue.size                       | healthwatch                   |                         | Healthwatch         | UPPER          | 10000        |            | Number  | 5                           |
| Scaling Alert: Redis Value Metric Queue Size                               | redis.valueMetricQueue.size                        | healthwatch                   |                         | Healthwatch         | UPPER          | 10000        |            | Number  | 5                           |
| Performance Alert: Cell Rep Time to Sync                                   | RepBulkSyncDuration                                | rep                           |                         | Compute Performance | UPPER          | 10000000000  | 5000000000 | ns      | 15                          |
| Performance Alert: Number of Router 5XX Server Errors<sup>1</sup>          | responses.5xx                                      | gorouter                      |                         | Routing             | UPPER          | 40           | 30         | Number  | 5                           |
| Performance Alert: Route Emitter Time to Sync<sup>1</sup>                  | RouteEmitterSyncDuration                           | route_emitter                 |                         | Compute Performance | UPPER          | 10000000000  | 5000000000 | ns      | 15                          |
| Performance Alert: Number of Route Registration Messages Sent and Received | RouteRegistration.MessagesDelta                    | healthwatch                   |                         | Routing             | UPPER          | 50           | 30         | Number  | 5                           |
| Performance Alert: BBS Time to Handle Requests                             | RequestLatency                                     | bbs                           |                         | Compute Performance | UPPER          | 10000000000  | 5000000000 | ns      | 15                          |
| Performance Alert: UAA Requests In Flight                                  | server.inflight.count                              | uaa                           |                         | UAA                 | UPPER          | 200          | 150        | Number  | 5                           |
| Scaling Alert: Syslog Adapter Capacity                                     | SyslogDrain.Adapter.BindingsAverage.5M             | healthwatch                   |                         | Logging             | UPPER          | 500          | 450        | Number  | 60                          |
| Scaling Alert: Syslog Adapter Loss Rate                                    | SyslogDrain.Adapter.LossRate.1M                    | healthwatch                   |                         | Logging             | UPPER          | 0.1          | 0.01       | Percent | 5                           |
| Scaling Alert: Reverse Log Proxy Loss Rate                                 | SyslogDrain.RLP.LossRate.1M                        | healthwatch                   |                         | Logging             | UPPER          | 0.1          | 0.01       | Percent | 5                           |
| Scaling Alert: Router Instance CPU                                         | system.cpu.user                                    | bosh-system-metrics-forwarder | router                  | Routing             | UPPER          | 70           | 60         | Percent | 5                           |
| Scaling Alert: UAA Instance CPU                                            | system.cpu.user                                    | bosh-system-metrics-forwarder | uaa,control<sup>3</sup> | UAA                 | UPPER          | 90           | 80         | Percent | 5                           |
| Performance Alert: VM CPU                                                  | system.cpu.user                                    | bosh-system-metrics-forwarder |                         | All Jobs            | UPPER          | 95           | 85         | Percent | 5                           |
| Performance Alert: VM Ephemeral Disk Used                                  | system.disk.ephemeral.percent                      | bosh-system-metrics-forwarder |                         | All Jobs            | UPPER          | 90           | 80         | Percent | 5                           |
| Performance Alert: VM Persistent Disk Used                                 | system.disk.persistent.percent                     | bosh-system-metrics-forwarder |                         | All Jobs            | UPPER          | 90           | 80         | Percent | 5                           |
| Performance Alert: VM Disk Used                                            | system.disk.system.percent                         | bosh-system-metrics-forwarder |                         | All Jobs            | UPPER          | 90           | 80         | Percent | 5                           |
| Performance Alert: VM Health Check Recovery                                | system.healthy                                     | bosh-system-metrics-forwarder |                         | All Jobs            | LOWER          | 0.4          | 0.6        | Number  | 5                           |
| Performance Alert: Router Throughput<sup>1</sup>                           | total_requests                                     | gorouter                      |                         | Routing             | UPPER          | 125000       | 100000     | Number  | 5                           |
| Performance Alert: Number of Router Routes Registered<sup>1</sup>          | total_routes                                       | gorouter                      |                         | Routing             | UPPER          | 200          | 100        | Number  | 5                           |
| Performance Alert: VM Memory Used                                          | system.mem.percent                                 | bosh-system-metrics-forwarder |                         | All Jobs            | UPPER          | 95           | 85         | Percent | 5                           |
| Performance Alert: UAA Throughput Rate                                     | uaa.throughput.rate                                | healthwatch                   |                         | Healthwatch         | UPPER          | 15000        | 12000      | Number  | 5                           |
| Performance Alert: Unhealthy Cells<sup>2</sup>                             | UnhealthyCell                                      | rep                           |                         | Compute Performance | EQUALITY       | 0            |            | Number  | 5                           |

You can learn more about the metrics PCF Healthwatch emits here: [PCF Healthwatch Metrics](../metrics.html).

<sup>1</sup> Pivotal recommends customizing the default thresholds for these alerts based on your environment. You can determine the best threshold for your environment by monitoring the metrics over time and noting the metric values that indicate acceptable and unacceptable system performance and health. For more information about updating alert configurations in PCF Healthwatch, see [Update Alert Configurations](#post).<br/>
<sup>2</sup> We are alerting by cell for this metric. We will notify at a critical level when any Diego Cell has been unhealthy for 5 minutes.<br/>
<sup>3</sup> This handles two jobs to accommodate different job names between PAS and SRT*

##<a id='errors'></a>Errors

This section lists common error messages and their causes.

---
**Error Message**: "Unsupported condition in query expression"

**Possible Cause**: Key in query string is something other than `name`, `origin`, `job`, or `deployment`

---
**Error Message**: "Unsupported query expression"

**Possible Causes**:

- Query string does not include AT LEAST `name` and `origin`
- Found operator other than `and` or `==`
- Expression in query string not in format of `property == 'value' and ...`

---
**Error Message**: "Invalid query expression"

**Possible Cause**: Invalid query string format

---
**Error Message**: "Invalid threshold type for metric 'some_origin.some_name'")

**Possible Cause**: Given threshold type does not match expected (see table above)

---
**Error Message**: "Must provide a warning threshold for an Upper/Lower Threshold"

**Possible Cause**: Threshold missing required values for type

---
**Error Message**: "name = 'some_name' and origin = 'some_origin' is not a supported alert configuration"
**Error Message**: "name = 'some_name', origin = 'some_origin', and job = 'some_job' is not a supported alert configuration"

**Possible Cause**: An alert configuration does not exist for the targeted metric

##<a id='Example'></a>Walkthrough Example

A best practice deployment of Cloud Foundry includes at least three availability zones (AZs). For these types of deployments, Pivotal recommends that you have enough capacity to suffer failure of an entire AZ.

By default, Healthwatch sends an alert if the `Diego.TotalPercentageAvailableMemoryCapacity.5M` metric falls below 35%, or one in three.

However, if your environment has been scaled up to five AZs you may wish to adjust the alert configuration accordingly to 20%, or more in five.

```
uaac token client get <my_healthwatch_admin_client> -s <my_healthwatch_admin_secret>
export token=$(uaac context | grep access_token | awk '{print $2}')

uaac curl -X POST "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations"  \
      -H "Content-Type: application/json" \
      --data "{\"query\":\"origin == 'healthwatch' and name == 'Diego.TotalPercentageAvailableMemoryCapacity.5M'\",\"threshold\":{\"critical\":0.2,\"warning\":0.3,\"type\":\"LOWER\"}}"
```

The response body contains the updated alert configuration. You can then confirm the change:

```
uaac curl "https://healthwatch-api.SYSTEM-DOMAIN/v1/alert-configurations?q=origin == 'healthwatch' and name == 'Diego.TotalPercentageAvailableMemoryCapacity.5M'"
```
##<a id='event-alerts'></a>Configure PCF Healthwatch Alert Notifications

You can configure PCF Event Alerts to receive push notifications when a PCF Healthwatch alert occurs. For example, if you configured a PCF Healthwatch alert for memory on a VM, you can use PCF Event Alerts to receive a message on Slack if memory on the VM exceeds the threshold defined in the PCF Healthwatch alert.

For more information about configuring PCF Event Alerts for PCF Healthwatch, see [PCF Event Alerts](https://docs.pivotal.io/event-alerts/index.html).